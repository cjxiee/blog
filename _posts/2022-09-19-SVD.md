## Singular value decomposition

A = S $V^{-1}$ = S $V^{T}$

where V is the matrix of decomposition axes, S is the matrix of the len of projections.

Any set of vectors (A) can be expressed in terms of their lengths of projections (S) on some set of orthogonal axes (V).

![image](https://user-images.githubusercontent.com/89954165/191116308-edfa07e3-82be-44c3-bd31-66e476f9541d.png)

First, let’s name things. Recall that any pair of orthogonal vectors in two-dimensional space forms a basis for that space. In our case, let’s call the orthogonal vectors in the input space v1\textbf{v}_1v1​ and v2\textbf{v}_2v2​ (Figure 6A). After applying a matrix transformation MMM to these two vectors, we get Mv1M \textbf{v}_1Mv1​ and Mv2M \textbf{v}_2Mv2​ (Figure 6B). Furthermore, let’s decompose these two transformed vectors into unit vectors, u1\textbf{u}_1u1​ and u2\textbf{u}_2u2​, times their respective magnitudes, σ1\sigma_1σ1​ and σ2\sigma_2σ2​. In other words:

Mv1=u1σ1Mv2=u2σ2(2) \begin{aligned} M \textbf{v}_1 &= \textbf{u}_1 \sigma_1 \\ M \textbf{v}_2 &= \textbf{u}_2 \sigma_2 \end{aligned} \tag{2} Mv1​Mv2​​=u1​σ1​=u2​σ2​​(2)

So far, we have said nothing new. We are just naming things.


Figure 6: Formalizing the geometric essence of SVD: if we properly rotate our domain defined by the basis vectors v1\textbf{v}_1v1​ and v2\textbf{v}_2v2​, then any linear transformation MMM is just a transformation by a diagonal matrix (dilating, reflecting) in a potentially rotated range defined by u1\textbf{u}_1u1​ and u2\textbf{u}_2u2​.
But now that we have things named, we can do a little algebraic manipulation. First, note that any vector x\textbf{x}x can be described using the basis vectors v1\textbf{v}_1v1​ and v2\textbf{v}_2v2​ in the following way,

x=(x⋅v1)v1+(x⋅v2)v2(3) \textbf{x} = (\textbf{x} \cdot \textbf{v}_1) \textbf{v}_1 + (\textbf{x} \cdot \textbf{v}_2) \textbf{v}_2 \tag{3} x=(x⋅v1​)v1​+(x⋅v2​)v2​(3)

where a⋅b\textbf{a} \cdot \textbf{b}a⋅b denotes the dot product between vectors a\textbf{a}a and b\textbf{b}b. If you’re unsure about the above formulation, consider a similar formulation with the standard basis vectors:

x=x1[10]+x2[01] \textbf{x} = x_1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + x_2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} x=x1​[10​]+x2​[01​]

where xix_ixi​ denotes the iii-th component of x\textbf{x}x. In Equation 333, we are projecting, via the dot product, x\textbf{x}x onto v1\textbf{v}_1v1​ and v2\textbf{v}_2v2​, before decomposing the terms using the basis vectors v1\textbf{v}_1v1​ and v2\textbf{v}_2v2​.

Next, let’s left-multiply both sides of Equation 333 by our matrix transformation MMM. Since the dot product produces a scalar, we can distribute and commute MMM as follows:

Mx=(x⋅v1)Mv1+(x⋅v2)Mv2 M \textbf{x} = (\textbf{x} \cdot \textbf{v}_1) M \textbf{v}_1 + (\textbf{x} \cdot \textbf{v}_2) M \textbf{v}_2 Mx=(x⋅v1​)Mv1​+(x⋅v2​)Mv2​

Next, we can rewrite MviM \textbf{v}_iMvi​ as uiσi\textbf{u}_i \sigma_iui​σi​:

Mx=(x⋅v1)u1σ1+(x⋅v2)u2σ2 M \textbf{x} = (\textbf{x} \cdot \textbf{v}_1) \textbf{u}_1 \sigma_1 + (\textbf{x} \cdot \textbf{v}_2) \textbf{u}_2 \sigma_2 Mx=(x⋅v1​)u1​σ1​+(x⋅v2​)u2​σ2​

We’re almost there. Now since the dot product is commutative, we can switch the ordering, e.g. x⋅v1=x⊤v1=v1⊤x\textbf{x} \cdot \textbf{v}_1 = \textbf{x}^{\top} \textbf{v}_1 = \textbf{v}_1^{\top} \textbf{x}x⋅v1​=x⊤v1​=v1⊤​x. And since each dot product term is a scalar, we move each one to the end of their respective expressions:

Mx=u1σ1v1⊤x+u2σ2v2⊤x M \textbf{x} = \textbf{u}_1 \sigma_1 \textbf{v}_1^{\top} \textbf{x} + \textbf{u}_2 \sigma_2 \textbf{v}_2^{\top} \textbf{x} Mx=u1​σ1​v1⊤​x+u2​σ2​v2⊤​x

Now we can remove x\textbf{x}x from both sides of the equation because, in general, Ax=Bx  ⟹  A=BA \mathbf{x} = B \mathbf{x} \implies A = BAx=Bx⟹A=B. For details, see the discussion here. It might be easier to intuit if we rewrite the claim as (A−B)x=0  ⟹  A−B=0(A - B) \mathbf{x} = 0 \implies A - B = 0(A−B)x=0⟹A−B=0. Removing x\mathbf{x}x, we get:

M=u1σ1v1⊤+u2σ2v2⊤(4) M = \textbf{u}_1 \sigma_1 \textbf{v}_1^{\top} + \textbf{u}_2 \sigma_2 \textbf{v}_2^{\top} \tag{4} M=u1​σ1​v1⊤​+u2​σ2​v2⊤​(4)

Given appropriately defined matrices, Equation 444 becomes SVD in its canonical form (Equation 111) for 2×22 \times 22×2 matrices:

$M=[[u1u2]⏟U[[σ100σ2]⏟Σ[[v1⊤v2⊤]⏟V⊤(5) M = \underbrace{\vphantom{\Bigg[}\begin{bmatrix} \textbf{u}_1 & \textbf{u}_2 \end{bmatrix}}_{\textstyle U} \underbrace{\vphantom{\Bigg[}\begin{bmatrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{bmatrix}}_{\textstyle \Sigma} \underbrace{\vphantom{\Bigg[}\begin{bmatrix} \textbf{v}_1^{\top} \\ \textbf{v}_2^{\top} \end{bmatrix}}_{\textstyle V^{\top}} \tag{5} M=U[[u1​​u2​​]​​Σ[[σ1​0​0σ2​​]​​V⊤[[v1⊤​v2⊤​​]​​(5)$

Furthermore, you should have an intuition for what this means. Any matrix transformation can be represented as a diagonal transformation (dilating, reflecting) defined by Σ\SigmaΣ provided the domain and range are properly rotated first.

The vectors ui\textbf{u}_iui​ are called the left singular vectors, while the vectors vi\textbf{v}_ivi​ are called the right singular vectors. This orientating terminology is a bit confusing because “left” and “right” come from the equation above, while in our diagrams of rectangles and ellipses, the vi\textbf{v}_ivi​ vectors are on the left.

The standard formulation
Now that we have a simple geometrical intuition for the SVD and have formalized it for all 2×22 \times 22×2 matrices, let’s re-formulate the problem in general and in the standard way. If jumping from 222-dimensions to nnn-dimensions is challenging, recall the advice of Geoff Hinton:

To deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say “fourteen” to yourself very loudly. Everyone does it.

Consider that given our definitions of vi\textbf{v}_ivi​, ui\textbf{u}_iui​, and σi\sigma_iσi​, we can rewrite Equation 222 for an arbitrary m×nm \times nm×n matrix MMM as:

[M][v1v2…vn]=[u1u2…um][σ1σ2⋱σn] $\begin{bmatrix} \\ \\ & & M & & \\ \\ \\ \end{bmatrix} \left[\begin{array}{c|c|c|c} \\ \textbf{v}_1 & \textbf{v}_2 & \dots & \textbf{v}_n \\ \\ \end{array}\right] = \left[\begin{array}{c|c|c|c} \\ \\ \textbf{u}_1 & \textbf{u}_2 & \dots & \textbf{u}_m \\ \\ \\ \end{array}\right] \begin{bmatrix} \sigma_1 & & & \\ & \sigma_2 & & \\ & & \ddots & \\ & & & \sigma_n \\ \\ \end{bmatrix} ⎣⎢⎢⎢⎢⎢⎡​​​M​​​⎦⎥⎥⎥⎥⎥⎤​⎣⎢⎡​v1​​v2​​…​vn​​⎦⎥⎤​=⎣⎢⎢⎢⎢⎢⎡​u1​​u2​​…​um​​⎦⎥⎥⎥⎥⎥⎤​⎣⎢⎢⎢⎢⎢⎡​σ1​​σ2​​⋱​σn​​⎦⎥⎥⎥⎥⎥⎤​$

which immediately gives us Equation 555 again, but for m×nm \times nm×n matrices:

MV=UΣMVV∗=UΣV∗M=UΣV∗ \begin{aligned} M V &= U \Sigma \\ M V V^{*} &= U \Sigma V^{*} \\ M &= U \Sigma V^{*} \end{aligned} MVMVV∗M​=UΣ=UΣV∗=UΣV∗​

Here, UUU is unitary and m×mm \times mm×m; VVV is unitary and n×nn \times nn×n; and Σ\SigmaΣ is diagonal and m×nm \times nm×n. VV∗=IVV^{*} = IVV∗=I since VVV is orthonormal. This is schematically drawn in Figure 7.
